**=====================================================**

# Machine Failure Analysis 
Noah MacGillivray | BIT-446 Assignment(T5) | Summer 2025 

Model Evaluation of a KNN Classification Model

**Environment:** Ubuntu, VS Code

**=====================================================**

## Introduction
This program imports machine failure data for three different models with (possible)
associated factors including hours run and average hours in-between maintenance. The 
data is prepared, splits for training and testing are made using both random 
stratified and stratified k-folds, and the k nearest neighbor (KNN) algorithm is 
applied to both. The results are then displayed for evaluation.   


## Required packages 
```{r}
# Load packages
library(readxl)
library(skimr)
library(dplyr)
library(class)
library(caTools)
library(caret)
library(ggplot2)
library(lattice)
library(metrica)

cat("All packages loaded successfully!\n")
```

## Data Import and Initial Exploration 
The machine failure data is read in using read_excel, head/tail/summary are printed to
ensure proper import and view summary, and machine version prints what percentage of 
failures are in each group. The first 300 rows of dataset had no missing values for 
columns needed for KNN evaluation.    
```{r}
failure_rate <- read_excel("BIT-446-RS-T4-T5-FailureRate.xlsx", n_max = 300)

# Print head
head(failure_rate)

# Print tail
tail(failure_rate)

# Print skimr dataset summary
skim(failure_rate)

# Machine number failure rate summary
model_version_summary <- failure_rate %>%
  group_by(model_version) %>%
  summarise(
    total_units = n(),
    failure_rate = mean(failure),
    .groups = "drop"
  )
cat("Machine Model Failure Distribution and Failure Rate")

# Prints the summary
print(model_version_summary)
```

## General impressions from summary 
- 300 column vectors with 5 dimension. 
- hours_run: uniform distribution with moderate standard deviation 
- avg_hours_between_maint: left-skewed with moderate standard deviation 
- failure: right skewed with high standard deviation 
- Model data is equally distributed; model 3 has slightly higher failure rate

## Data Preparation
Do to the large range of values, hours_run and avg_hours_between_maint were normalized
(z-score). The failure column is also converted to a factor to be used in the 
KNN models. 

```{r}
# Normalize only the features of interest; default is z-score
failure_rate$hours_run <- as.vector(scale(failure_rate$hours_run))

failure_rate$avg_hours_between_maint <-
  as.vector(scale(failure_rate$avg_hours_between_maint))

# Failure column converted to a factor 
failure_rate$failure <- as.factor(failure_rate$failure)
```

## Data splitting for ***Stratified*** KNN training 
Seed was set for reproducibility; data was split using random stratified approach 
(random selection, but each train/test division has equal number of failed machines) -
70% for training and 30% for testing.  
```{r}
# Set seed for reproducible results
set.seed(44)

# Stratified random split 70/30
sample <- sample.split(failure_rate$failure, SplitRatio = 0.7)

# Distribute the split data to training and testing
train_failure <- subset(failure_rate, sample == TRUE)
test_failure <- subset(failure_rate, sample == FALSE)
```

## Apply KNN (***Stratified***)
This bock applies the KNN algorithm based on hours_run and avg_hours_between_maint
features and failure target. A k-value of 5 was selected based on our last analysis
using KNIME. The function returns an object of failure criteria to be compared to 
the known failures of the associated column vector. 
```{r}
# Apply KNN classification with hours_run and avg_hours_between_maint
failure_knn <- knn(
  train_failure[c("hours_run", "avg_hours_between_maint")],
  test_failure[c("hours_run", "avg_hours_between_maint")],
  cl = train_failure$failure, # Class labels
  k = 5
)
```

## Data splitting for ***k-folds*** KNN training 
The data splitting for k-folds is a bit different than using a stratified approach. 
It still uses a seed for reproducibility and a stratified approach when placing data 
into each fold, but builds 10 separate folds (tests) to apply the KNN model. This 
block also creates a column vector of specified k-values to be tested along with the 
dataset below. 

```{r}
# Set seed for reproducibility
set.seed(44)

# Set up 10 fold cross validation via caret package
ctrl <- trainControl(
  method = "cv",           # Cross validation
  number = 10,             # 10 folds
  savePredictions = "final" # Save predictions for confusion matrix
)

# Define k values to test
k_grid <- data.frame(k = c(3, 5, 7, 9, 11, 13, 15, 17))
```


## Apply KNN (***k-folds***)
This block uses the caret library and is where the k_NN model is applied to the 10 
separate folds and each of the k-values in the vector above. It averages the outcome 
accuracy and other metrics of the 10 different folds. This allows you to test and verify
more of your data, prevent overfitting and hopefully provide better results on future
data. 

```{r}
# Apply KNN with k-fold cross-validation
knn_kfolds_model <- train(
  ## Predict failure based on hours_run + avg_hours_between_maint
  failure ~ hours_run + avg_hours_between_maint,
  # Data set we are using (from above)
  data = failure_rate,
  # KNN algorithm applied
  method = "knn",
  # Number of folds and cross validation we set above
  trControl = ctrl,
  # Test for each of the specified k-values above
  tuneGrid = k_grid,
  # Optimize for accuracy predicting failure
  metric = "Accuracy"
)
```


## Model Evaluation Metrics (***stratified***)
The following block binds the applied KNN model above with our test_failure column
to provide a comparison on how well the KNN model performed. Displays include: 
confusion matrix, comprehensive metrics summary, and a scatter plot of the predictions.  
```{r}
# Bind the KNN model with our data frame
test_obs_pred <- data.frame(test_failure, failure_knn)

# Display confusion matrix
confusionMatrix(test_obs_pred$failure_knn,
                test_obs_pred$failure,
                mode = "prec_recall")

# Converts factors back to numerical values and
# places them in a df to use w/ metric summary
obs <- as.numeric(paste(test_failure$failure))
pred <- as.numeric(paste(test_obs_pred$failure_knn))
data <- data.frame(cbind(obs, pred))

# Create comprehensive metrics summary using metrica package
metrics_summary(
  data = data,
  obs = obs,
  pred = pred,
  pos_level = 1,
  type = "classification"
)

# Scatter plot
ggplot(test_obs_pred, aes(x = hours_run, y = avg_hours_between_maint, 
                          color = failure == failure_knn)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("FALSE" = "red", "TRUE" = "blue"),
                     name = "Prediction", labels = c("Wrong", "Correct")) +
  labs(title = "KNN Machine Failure Predictions (Stratified Approach)",
       x = "Hours Run (normalized)",
       y = "Avg Hours Between Maintenance(normalized)") +
  theme_minimal()
```

## Model Evaluation Metrics (***k-folds***)
The following block prints and plots the k-value metrics from KNN model to provide 
a comparison on how well different k-values performed. It also displays the best k-value
using the caret library. Other displays include: confusion matrix, comprehensive 
metrics summary, and a scatter plot of the predictions.

```{r}
# View results
print(knn_kfolds_model)

# Plot k value performance
plot(knn_kfolds_model, main = "K-ValuePerformance in KNN Model")

# Get the best k value
cat("Best k value:", knn_kfolds_model$bestTune$k, "\n")

# Confusion matrix for best model compiles across all folds
caret_cm <- confusionMatrix(knn_kfolds_model$pred$pred,
                            knn_kfolds_model$pred$obs, mode = "prec_recall")
print(caret_cm)

# Comprehensive metrics using metrica package
obs_caret <- as.numeric(paste(knn_kfolds_model$pred$obs))
pred_caret <- as.numeric(paste(knn_kfolds_model$pred$pred))
caret_data <- data.frame(cbind(obs = obs_caret, pred = pred_caret))

metrics_summary(
  data = caret_data,
  obs = obs_caret,
  pred = pred_caret,
  pos_level = 1,
  type = "classification"
)

pred_indices <- knn_kfolds_model$pred$rowIndex

# Create data frame for plotting
kfold_plot_data <- data.frame(
  hours_run = failure_rate$hours_run[pred_indices],
  avg_hours_between_maint = failure_rate$avg_hours_between_maint[pred_indices],
  actual = knn_kfolds_model$pred$obs,
  predicted = knn_kfolds_model$pred$pred,
  correct = knn_kfolds_model$pred$obs == knn_kfolds_model$pred$pred
)

# Scatter plot
ggplot(kfold_plot_data,
       aes(x = hours_run, y = avg_hours_between_maint, color = correct)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("FALSE" = "red", "TRUE" = "blue"),
                     name = "Prediction", labels = c("Wrong", "Correct")) +
  labs(title = "KNN Machine Failure Predictions 
      (K-Fold Cross-Validation Approach)",
       subtitle = paste("Best k =", knn_kfolds_model$bestTune$k, "| Accuracy =",
                        round(max(knn_kfolds_model$results$Accuracy), 3)),
       x = "Hours Run (normalized)",
       y = "Avg Hours Between Maintenance (normalized)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```

## Conclusion and analysis 
***After analysis of the KNN model of machine failure, yes it should be used to predict
machine failure, and specifically, we should use the k-folds method with a k-value of 
13.*** 

As we can see, a k-value of 13 gave us the best accuracy and Kappa scores. Although 
k=11 appeared to have the same values, it appears (after looking into it) either 
the caret library either chose 13 because of rounding that we did not see in the metrics,  
or other metrics it compared. The k-folds method not only produces a different/better
k-value, but it also outperformed the stratified approach on ever other metric we 
investigated.

The metrics for the k-fold approach to KNN showed promising results. The model showed
a 97% accuracy rate and a Kappa score of 91% (better than random guessing). There was
high precision showing that 97% of failures recognized were correct; high recall showing
99% of the total failures were recognized; and a harmonic mean (F1) of those two metrics
was 98%. 

The error rate metrics were also promising for a business model detecting failures 
related to costs of failed machines, false alarms, and incorrect detection. The following
is noted:

- FPR(false positive rate): Of all the machines that don't fail, 11% are marked as failed. 
- FNR(false negative rate): We miss 0.8% of the failed machines. 
- FDR(false discovery rate): 2% of flagged as failed machines are false alarms. 
- FOR(False Omission rate): 3% of machines we label as "OK" are actually faulty. 

Of these metrics, the false omission rate may be the most important depending on what 
the machine is used for. If the machine is safety sensitive and may cause great harm 
to health, another safety measure may need to be set in place to catch as many of the 
3% as possible. 

**Further discussion**

I wanted to run more tests (did not have time) isolating the hours run and maintenance 
intervals to see if one of those factors significantly showed a stronger corelation of 
machine failure. There is an obvious correlation of hours run and typical maintenance, 
but it would be interesting to see if one of those has more weight, or if there is a 
threshold of hours run to take that machine out of service. 





