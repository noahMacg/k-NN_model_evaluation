**=====================================================**

# Machine Failure Analysis 
Noah MacGillivray | BIT-446 Assignment(T5) | Summer 2025 

Model Evaluation of a k-NN Classification Model

**Environment:** Ubuntu, VS Code

**=====================================================**

## Required packages 
```{r}
# Load packages
library(readxl)
library(skimr)
library(dplyr)
library(class)
library(caTools)
library(caret)
library(ggplot2)
library(lattice)
library(metrica)

cat("All packages loaded successfully!\n")
```

## Import the first 300 rows and print dataset summary
```{r}
failure_rate <- read_excel("BIT-446-RS-T4-T5-FailureRate.xlsx", n_max = 300)

# Print head
head(failure_rate)

# Print tail
tail(failure_rate)

# Print skimr dataset summary
skim(failure_rate)

# Machine number failure rate summary
model_version_summary <- failure_rate %>%
  group_by(model_version) %>%
  summarise(
    total_units = n(),
    failure_rate = mean(failure),
    .groups = "drop"
  )

print(model_version_summary)
```

## General impressions from summary 


## Data preparation for k-NN training 

```{r}
# Normalize only the features of interest; default is z-score
failure_rate$hours_run <- as.vector(scale(failure_rate$hours_run))

failure_rate$avg_hours_between_maint <-
  as.vector(scale(failure_rate$avg_hours_between_maint))

# Set seed for reproducible results
set.seed(44)

# Stratified random split 70/30
sample <- sample.split(failure_rate$failure, SplitRatio = 0.7)

# Distribute the split data to training and testing
train_failure <- subset(failure_rate, sample == TRUE)
test_failure <- subset(failure_rate, sample == FALSE)

# Converts data sets to categorical factors for proper handling in k-NN model
train_failure$failure <- as.factor(train_failure$failure)
test_failure$failure <- as.factor(test_failure$failure)

```

## K-NN evaluation (stratified) 
```{r}
# Apply k-NN classification with hours_run and avg_hours_between_maint
failure_knn <- knn(
  train_failure[c("hours_run", "avg_hours_between_maint")],
  test_failure[c("hours_run", "avg_hours_between_maint")],
  cl = train_failure$failure,
  k = 5
)
```

## K-NN evaluation (k-folds)

```{r}
# Set seed for reproducibility
set.seed(44)

# Set up 10-fold cross-validation
ctrl <- trainControl(
  method = "cv",           # Cross-validation
  number = 10,             # 10 folds
  savePredictions = "final" # Save predictions for confusion matrix
)

# Define k values to test
k_grid <- data.frame(k = c(3, 5, 7, 9, 11, 13))

failure_rate$failure <- as.factor(failure_rate$failure)

# Train k-NN with k-fold cross-validation
knn_model <- train(
  failure ~ hours_run + avg_hours_between_maint,
  data = failure_rate,
  method = "knn",
  trControl = ctrl,
  tuneGrid = k_grid,
  metric = "Accuracy"
)

# View results
print(knn_model)

# Plot k value performance
plot(knn_model)

# Get the best k value
cat("Best k value:", knn_model$bestTune$k, "\n")
cat("Best ROC:", round(max(knn_model$results$ROC), 4), "\n")

# Detailed results for each k value
print(knn_model$results)

# Confusion matrix for best model compiles across all folds
confusionMatrix(knn_model$pred$pred, knn_model$pred$obs)

# Detailed results for each k value
print(knn_model$results)

# Confusion matrix for best model compiles across all folds
caret_cm <- confusionMatrix(knn_model$pred$pred,
                            knn_model$pred$obs, mode = "prec_recall")
print(caret_cm)

# Comprehensive metrics using metrica package
obs_caret <- as.numeric(paste(knn_model$pred$obs))
pred_caret <- as.numeric(paste(knn_model$pred$pred))
caret_data <- data.frame(cbind(obs = obs_caret, pred = pred_caret))

metrics_summary(
  data = caret_data,
  obs = obs_caret,
  pred = pred_caret,
  pos_level = 1,
  type = "classification"
)
```

## Model Evaluation Metrics 
```{r}
# Bind the k-NN model with our data frame
test_obs_pred <- data.frame(test_failure, failure_knn)

# Display confusion matrix
confusionMatrix(test_obs_pred$failure_knn,
                test_obs_pred$failure,
                mode = "prec_recall")

# Comprehensive metrics summary using metrica package
obs <- as.numeric(paste(test_failure$failure))
pred <- as.numeric(paste(test_obs_pred$failure_knn))
data <- data.frame(cbind(obs, pred))

# Create comprehensive metrics summary using metrica package
metrics_summary(
  data = data,
  obs = obs,
  pred = pred,
  pos_level = 1,
  type = "classification"
)

# Provide model results using caret package
confusionMatrix(
  data = factor(pred),
  reference = factor(obs),
  mode = "prec_recall"
)
# Scatter plot 
ggplot(test_obs_pred, aes(x = hours_run, y = avg_hours_between_maint, 
                          color = failure == failure_knn)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("FALSE" = "red", "TRUE" = "blue"),
                     name = "Prediction", labels = c("Wrong", "Correct")) +
  labs(title = "k-NN Machine Failure Predictions", 
       x = "Hours Run (normalized)",
       y = "Avg Hours Between Maintenance(normalized)") +
  theme_minimal()
```

## Conclusion and analysis 